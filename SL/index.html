<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Statistical Learning</title>
<link rel="shortcut icon" href="favicon.ico" />
<meta name="Description" content="Course material for STA 531"/>
<meta name="Keywords" content="statistics, machine learning, probability, inference, mathematics, math, applied math" /> 
<meta name="author" content="Jeff Miller"/> 
<meta name="robots" content="index,follow" />
<!-- <meta name="language" content="English" /> -->
</head>


<body>

<h1>Statistical Learning</h1>

<h2>Course material for BST 263</h2>
Instructor: Jeff Miller<br>
Spring 2019<br>
Harvard T.H. Chan School of Public Health<br>
Department of Biostatistics<br>


<h3>Synopsis</h3>
Statistical learning is a collection of flexible tools and techniques for using data to construct prediction algorithms and perform exploratory analysis. This course will introduce students to the theory and application of methods for supervised learning (classification and regression) and unsupervised learning (dimension reduction and clustering). Students will learn the mathematical foundations underlying the methods, as well as how and when to apply different methods. Topics will include the bias-variance tradeoff, cross-validation, linear regression, logistic regression, KNN, LDA/QDA, variable selection, penalized regression, generalized additive models, CART, random forests, gradient boosting, kernels, SVMs, PCA, and K-means. Homework will involve mathematical and programming exercises, and exams will contain conceptual and mathematical problems. Programming in R will be used throughout the course to provide hands-on training and practical examples.


<h3>General information</h3>

<ul>
<li> <a href="SL-syllabus.pdf">Syllabus</a> </li>
<li> Textbooks: </li>
    <ul>
    <li> 
    <a href="http://www-bcf.usc.edu/~gareth/ISL/"> 
    An Introduction to Statistical Learning</a>, by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
    </li>
    <li>
    <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"> 
    The Elements of Statistical Learning</a>, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
    </li>
    </ul>
</ul>


<h3>Lecture notes</h3>
<ul>
<li> <a href="1-Introduction.pdf">1. Introduction</a> (Course overview, Choosing among methods) </li>
<li> <a href="2-Probability-and-Linear-algebra.pdf">2. Probability and linear algebra basics</a></li>
<li> <a href="3-Measuring-performance.pdf">3. Measuring performance</a> (K-nearest neighbors, MSE, Bias-variance, Classification error rate, Bayes optimal) </li>
    <ul>
    <li> <a href="knn.r">knn.r</a> (R code for KNN regression, MSE, Bias-variance tradeoff) </li>
    <li> <a href="knn-classifier.r">knn-classifier.r</a> (R code for KNN classifier, Error rate, Bayes optimal classifier) </li>
    </ul>
<li> <a href="4-Lab-on-KNN.zip">4. Lab on KNN and measuring performance</a> </li>
<li> <a href="5-Linear-regression.pdf">5. Linear regression</a> (Probabilistic model, Basis functions, Estimation, Uncertainty quantification) </li>
<li> (In progress) </li>
</ul>




<h3>Homework assignments</h3>
<ul>
<li> <a href="homework-1.pdf">Homework 1</a> (Probability and linear algebra basics)</li>
<li> <a href="homework-2.pdf">Homework 2</a> (Measuring performance, Bias-variance)</li>
</ul>

<h3>Exams</h3>
<!---
<ul>
<li> <a href="midterm.pdf">Midterm</a> </li>
<li> <a href="midterm-solution.pdf">Midterm solutions</a> </li>
</ul>
--->

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>



</body>
</html>



